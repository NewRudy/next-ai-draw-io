# AI Provider Configuration
# AI_PROVIDER: Which provider to use
# Options: bedrock, openai, anthropic, google, azure, ollama, openrouter
# Default: bedrock
AI_PROVIDER=bedrock

# AI_MODEL: The model ID for your chosen provider (REQUIRED)
AI_MODEL=global.anthropic.claude-sonnet-4-5-20250929-v1:0

# AWS Bedrock Configuration
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=your-access-key-id
# AWS_SECRET_ACCESS_KEY=your-secret-access-key

# OpenAI Configuration
# OPENAI_API_KEY=sk-...
# 
# For ModelScope (魔塔社区), use ModelScope Token instead:
# MODELSCOPE_TOKEN=ms-xxxxx  # ModelScope Token (starts with ms-)
# OPENAI_BASE_URL=https://api-inference.modelscope.cn/v1  # Optional, for OpenAI-compatible APIs (e.g., ModelScope)
# AI_MODEL=ZhipuAI/GLM-4.6  # Model ID for ModelScope
# 
# Note: Do NOT include /chat/completions in baseURL, SDK will append it automatically
# 
# ModelScope Troubleshooting:
# - If you get "has no provider supported" error:
#   1. Check if the model is available at https://modelscope.cn
#   2. Verify your ModelScope Token has permission to access the model
#   3. Try alternative models: ZhipuAI/GLM-4.5, Qwen/Qwen2.5-72B-Instruct
#   4. Some models may require paid subscription or special permissions
# 
# Alternative: For ZhipuAI GLM-4.6 direct API (not ModelScope):
# OPENAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4
# OPENAI_API_KEY=your-zhipu-api-key  # Use ZhipuAI API key, not ModelScope Token
# OPENAI_ORGANIZATION=org-...  # Optional
# OPENAI_PROJECT=proj_...      # Optional

# Anthropic (Direct) Configuration
# ANTHROPIC_API_KEY=sk-ant-...

# Google Generative AI Configuration
# GOOGLE_GENERATIVE_AI_API_KEY=...

# Azure OpenAI Configuration
# AZURE_RESOURCE_NAME=your-resource-name
# AZURE_API_KEY=...

# Ollama (Local) Configuration
# OLLAMA_BASE_URL=http://localhost:11434/api  # Optional, defaults to localhost

# OpenRouter Configuration
# OPENROUTER_API_KEY=sk-or-v1-...
